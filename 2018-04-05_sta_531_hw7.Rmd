---
title: "Sta 531 HW7 (let it be the last one, please, let it end)"
author: "Daniel Truver"
date: "04/05/2018"
header-includes:
  - \usepackage{amsmath}
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### (1) Some Markov Chain

#### (a) Communicating Classes 

We say node $i$ communicates with $j$ if $\exists n$ such that $P(X_n = i \mid X_0 = j)$
We want to identify all pairs of nodes $\{i, j\}$ such that $i$ communicates with $j$ AND $j$ communicates with $i$. Nodes do not constitute a communicating class if the communication only goes one way. I remember this distinction by thinking about my ex-girlfriend. 

$\{1,5\}$ is a communicating class, as is $\{2,4\}$ is a communicating class.

##### (b) Recurrent and Transient States

States 1, 3, and 5 are recurrent; states 2 and 4 are transient. 

##### (c) Invariant Distributions 

We need to find a probability vector $\pi$ such that 
$$
\pi P = \begin{bmatrix} \pi_1&\pi_2&\pi_3&\pi_4&\pi_5 \end{bmatrix}P = \begin{bmatrix} \pi_1&\pi_2&\pi_3&\pi_4&\pi_5 \end{bmatrix} = \pi.
$$

We get the following system of equations:
$$
\begin{aligned}
\pi_1 &= \pi_1/2 + \pi_5/2\\
\pi_2 &= \pi_2/2 + \pi_4/2\\
\pi_3 &= \pi_3 + \pi_4/4 \\
\pi_4 &= \pi_2/2 + \pi_4/4 \\
\pi_5 &= \pi_1/2 + \pi_4/4 + \pi_5/2
\end{aligned}
$$

When we solve this system of equations, we get 
$$
\begin{aligned}
&\pi = \begin{bmatrix} x&0&y&0&x \end{bmatrix} \\
&x\in\left[0,\frac{1}{2}\right]\\
&y = 1-2x
\end{aligned}
$$

#### (2) Another Markov Chain

Let $X_0, X_1, X_2, \dots$ be a markov chain with tranistion matrix $P$. Then, if we take $Y_t = X_{kt}$, we get $Y_0 = X_0$ so the initial distribution of $Y_t$ is the same as $X_t$. We can get the markov property of $Y_t$ from the markov property of $X_t$.

$$
\begin{aligned}
P(Y_t = j\mid Y_{t-1} = i_1, Y_{t-2} = i_{2}, \dots) 
&= P(X_{kt} = j \mid X_{k(t-1)} = i_1, X_{k(t-2)}=i_2) \\
&=  P(X_{kt} = j \mid X_{k(t-1)} = i_1) = P_{ij}^{(k)} \\
&= P(Y_t = j\mid Y_{t-1} = i_1)
\end{aligned}
$$

The third line gives us the markov property $Y_t$; the second line gives us the transition probabilities, so the transition matrix of $Y_t$ is $P^k$.

#### (3) Some Proofs

##### (a) Show aperiodicity 

Suppose we have a finite state markov chain with the property that $\exists i: P_{ii} > 0$ AND it is irreducible. This tells us that the entire chain is one communicating class and there is a state with period = 1. And that means, say it with me, every state in the chain has period = 1. So it's aperiodic.

##### (b) Aperiodic + Irreducible = Regular

I need an additional idea that I do not recall from class. Without this outside fact, I would not have been able to prove this, especially not with the IN-CLASS test approaching.

I need a theorem that says for an aperiodic Markov chain, $\exists N$ such that $\forall n \geq N:P^n_{ii} > 0$; the number theory involved in this escapes me. 

Together with irreducibility, we have that for each $i,j$, there is an integer $N_{ij}$ such that $\forall n \geq N_{ij}:P^n_{ij} > 0$. Take $N = \max N_{ij}$. By the number theory theorem above, for each $i$ there is an integer $M_{ii}$ such that forall $m \geq M_{ii}:P^m_{ii} > 0$. Take $M = \max M_{ii}$. We then invoke the Chapman Kolmogorov equation.

$$
P^{M+N}_{ij} = \sum_k P_{ik}^M P_{kj}^N \geq P_{ii}^MP_{kj}^N > 0
$$

#### (4) Timmy's Books

We will model Timmy's book choosing process with the following markov chain described by its martix.
$$
P = \bordermatrix{
    & 123 & 132 & 213 & 231 & 312 & 321 \cr
    123 & \alpha_1 & 0 & \alpha_2 & 0 & \alpha_3 & 0 \cr
    132 & 0 & \alpha_1 & \alpha_2 & 0 & \alpha_3 & 0 \cr
    213 & \alpha_1 & 0 & \alpha_2 & 0 & 0 & \alpha_3 \cr
    231 & \alpha_1 & 0 & 0 & \alpha_2 & 0 & \alpha_3 \cr
    312 & 0 & \alpha_1 & 0 & \alpha_2 & \alpha_3 & 0 \cr
    321 & 0 & \alpha_1 & 0 & \alpha_2 & 0 & \alpha_3 
  }
$$

Assuming that the $\alpha_i$ are non-zero, this is an irreducible Markov Chain; it is aperiodic by definition. These conditions are sufficient to say it has a stationary distribution. That is, $p_n$ converges as $n\to\infty$. 